# ğŸ¤– Smart Chatbot - Tu Asistente de ProgramaciÃ³n

Un chatbot inteligente y funcional al 100% que utiliza **Ollama** para modelos de IA locales y se conecta a tu **repositorio de GitHub** para proporcionar asistencia contextual de programaciÃ³n.

## âœ¨ CaracterÃ­sticas Principales

- ğŸš€ **Chat en tiempo real** con WebSockets
- ğŸ§  **IntegraciÃ³n con Ollama** para modelos de IA locales
- ğŸ”— **ConexiÃ³n directa con GitHub** para anÃ¡lisis de repositorios
- ğŸ’¬ **Interfaz web moderna y responsive** 
- ğŸ“± **DiseÃ±o adaptativo** para mÃ³viles y escritorio
- ğŸ”„ **Estado del sistema en tiempo real**
- ğŸ“Š **AnÃ¡lisis de repositorios** con informaciÃ³n detallada
- âš¡ **Respuestas rÃ¡pidas** y contextuales

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Backend**: FastAPI (Python)
- **Frontend**: HTML5, CSS3, JavaScript (Vanilla)
- **IA**: Ollama (modelos locales)
- **GitHub**: PyGithub API
- **ComunicaciÃ³n**: WebSockets
- **Estilo**: CSS Grid, Flexbox, Gradientes

## ğŸ“‹ Requisitos Previos

1. **Python 3.8+** instalado
2. **Ollama** instalado y ejecutÃ¡ndose
3. **Token de GitHub** (opcional, para funcionalidad completa)

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio

```bash
git clone <tu-repositorio>
cd smart-chatbot
```

## ğŸŒ **Deploy en la Nube**

### **âš ï¸ IMPORTANTE: Limitaciones de Vercel**
Este chatbot usa **WebSockets** y **conexiones persistentes**, que **NO son compatibles** con Vercel (plataforma serverless).

**Plataformas recomendadas:**
- ğŸš‚ **Railway** - Soporta WebSockets (Recomendado)
- ğŸŒŠ **Render** - Soporta aplicaciones Python completas
- ğŸ¯ **Heroku** - Plataforma tradicional para Python

**Ver archivo `VERCEL_DEPLOY.md` para mÃ¡s detalles.**

### 2. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 3. Configurar Variables de Entorno

Copia el archivo de ejemplo y configÃºralo:

```bash
copy env.example .env
```

Edita el archivo `.env` con tus configuraciones:

```env
# ConfiguraciÃ³n de Ollama
OLLAMA_BASE_URL=http://localhost:11434

# ConfiguraciÃ³n de GitHub (opcional)
GITHUB_TOKEN=your_github_token_here
GITHUB_REPO=username/repository
```

### 4. Instalar y Configurar Ollama

#### Windows:
```bash
# Descargar desde: https://ollama.ai/download
# O usar winget:
winget install Ollama.Ollama
```

#### macOS:
```bash
brew install ollama
```

#### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 5. Descargar un Modelo

```bash
# Iniciar Ollama
ollama serve

# En otra terminal, descargar un modelo
ollama pull llama2
# o
ollama pull codellama
# o
ollama pull mistral
```

### 6. Ejecutar el Chatbot

```bash
python main.py
```

El chatbot estarÃ¡ disponible en: **http://localhost:8000**

## ğŸ”§ ConfiguraciÃ³n de GitHub (Opcional)

Para conectar tu repositorio de GitHub:

1. Ve a [GitHub Settings > Tokens](https://github.com/settings/tokens)
2. Genera un nuevo token con permisos de `repo`
3. Copia el token en tu archivo `.env`
4. Reinicia el chatbot

## ğŸ“± Uso del Chatbot

### Chat BÃ¡sico
- Escribe tu pregunta en el campo de texto
- Presiona Enter o haz clic en el botÃ³n de enviar
- El chatbot responderÃ¡ usando el modelo de Ollama

### ConexiÃ³n con GitHub
1. Ingresa la URL de tu repositorio en el sidebar
2. Haz clic en "Conectar"
3. El chatbot analizarÃ¡ tu repositorio
4. PodrÃ¡s hacer preguntas especÃ­ficas sobre tu cÃ³digo

### Ejemplos de Preguntas
- "Â¿CÃ³mo optimizar este cÃ³digo Python?"
- "Explica esta funciÃ³n de JavaScript"
- "Â¿CuÃ¡les son las mejores prÃ¡cticas para este patrÃ³n?"
- "AyÃºdame a debuggear este error"

## ğŸ—ï¸ Estructura del Proyecto

```
smart-chatbot/
â”œâ”€â”€ main.py              # AplicaciÃ³n principal FastAPI
â”œâ”€â”€ requirements.txt     # Dependencias de Python
â”œâ”€â”€ env.example         # Variables de entorno de ejemplo
â”œâ”€â”€ README.md           # Este archivo
â”œâ”€â”€ templates/          # Plantillas HTML
â”‚   â””â”€â”€ index.html     # Interfaz principal
â””â”€â”€ static/            # Archivos estÃ¡ticos (si los hay)
```

## ğŸ” Endpoints de la API

- `GET /` - Interfaz principal del chatbot
- `GET /api/health` - Estado del sistema
- `WS /ws` - WebSocket para chat en tiempo real

## ğŸ› SoluciÃ³n de Problemas

### Ollama no estÃ¡ ejecutÃ¡ndose
```bash
# Verificar si Ollama estÃ¡ corriendo
ollama list

# Si no estÃ¡ corriendo, iniciarlo
ollama serve
```

### Error de conexiÃ³n con GitHub
- Verifica que tu token sea vÃ¡lido
- AsegÃºrate de que tenga permisos de `repo`
- Revisa que la URL del repositorio sea correcta

### Problemas de puertos
- El chatbot usa el puerto 8000 por defecto
- Ollama usa el puerto 11434
- AsegÃºrate de que estos puertos estÃ©n disponibles

## ğŸš€ PersonalizaciÃ³n

### Cambiar el Modelo de Ollama
Edita la funciÃ³n `process_chat_message` en `main.py`:

```python
# Cambiar el modelo por defecto
model_name = "tu-modelo-preferido"
```

### Modificar la Interfaz
Edita `templates/index.html` para personalizar:
- Colores y estilos
- Layout y componentes
- Funcionalidades adicionales

### Agregar Nuevas Funcionalidades
- Nuevos endpoints en `main.py`
- Nuevos tipos de mensajes WebSocket
- Integraciones adicionales

## ğŸ“Š Monitoreo y Logs

El chatbot incluye:
- Indicadores de estado en tiempo real
- Logs de conexiÃ³n WebSocket
- VerificaciÃ³n de salud del sistema
- Manejo de errores robusto

## ğŸ”’ Seguridad

- Las variables sensibles se manejan a travÃ©s de archivos `.env`
- No se almacenan tokens en el cÃ³digo
- ValidaciÃ³n de entrada en todos los endpoints
- Manejo seguro de conexiones WebSocket

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ†˜ Soporte

Si tienes problemas:

1. Revisa la secciÃ³n de soluciÃ³n de problemas
2. Verifica que Ollama estÃ© ejecutÃ¡ndose
3. Revisa los logs de la consola
4. Abre un issue en el repositorio

## ğŸ¯ Roadmap

- [ ] Soporte para mÃºltiples modelos de Ollama
- [ ] Historial de conversaciones persistente
- [ ] AnÃ¡lisis de cÃ³digo mÃ¡s profundo
- [ ] IntegraciÃ³n con mÃ¡s plataformas de cÃ³digo
- [ ] API REST completa
- [ ] DockerizaciÃ³n
- [ ] Tests automatizados

---

**Â¡Disfruta usando tu Smart Chatbot personal! ğŸš€**

Si te gusta el proyecto, Â¡dale una estrella! â­

